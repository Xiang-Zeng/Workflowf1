#!/usr/bin/env python

# generates a hierarchical workflow

from Pegasus.DAX3 import *

import datetime
import getpass
import math
import os
import re
import socket
import sys
import time

from subprocess import call


def add_executables(dax):
    """
    adds executables for out wrappers in ./wrappers/
    """

    # Add executables to the DAX-level replica catalog
    for exe_name in os.listdir("./wrappers/"):
        exe = Executable(name=exe_name, arch="x86_64", installed=False)
        exe.addPFN(local_pfn(top_dir + "/workflow/wrappers/" + exe_name))
    
        if exe_name == "BIM":
            # BIM requires some extra memory
            exe.addProfile(Profile(Namespace.CONDOR, 
                                   key="request_memory",
                                   value="3 GB"))
        elif exe_name == "EVENT":
            # EVENT requires some extra disk
            exe.addProfile(Profile(Namespace.CONDOR, 
                                   key="request_disk",
                                   value="30 GB"))

        dax.addExecutable(exe)


def add_task_files(task_files, dax, job, lfn, base_dir, recursed=False):
    """
    add a set of input files from the task-files dir to a task
    """

    task_name = job.name

    if task_name not in task_files:
        task_files[task_name] = {}

    pfn = base_dir + "/" + lfn

    # do we have a directory or file?
    if os.path.isdir(pfn):
        # add all the entries
        for entry in os.listdir(pfn):
            new_lfn = lfn + "/" + entry
            add_task_files(task_files, dax, job, new_lfn, base_dir, True)
    else:
        # file
        if not os.path.isfile(pfn):
            print "ERROR: Input file does not exist: " + pfn
            sys.exit(1)
        if lfn not in task_files[task_name]:
            # do we already have the lfn registered for another task?
            f = None
            for tname in task_files:
                if lfn in task_files[tname]:
                    f = task_files[tname][lfn]
                    break
            if f is not None:
                task_files[task_name][lfn] = f
            else:
                task_files[task_name][lfn] = File(lfn)
                task_files[task_name][lfn].addPFN(local_pfn(pfn))
                #print(" ... registering input lfn/pfn: " + lfn + "  (" + pfn + ")")
                dax.addFile(task_files[task_name][lfn])

def local_pfn(path):
    """ generates a full pfn given a local path """
    pfn = None
    if exec_site == "condorpool":
        pfn = PFN("file://" + path, "local")
    else:
        pfn = PFN("scp://" + getpass.getuser() + "@" + socket.gethostname() + "/" + path, "local")
    return pfn

def get_building_count():
    """
    finds the number of buildings from inputs/buildings.csv
    """
    count = 0
    f = open("inputs/buildings.csv")
    while f.readline():
        count += 1
    f.close
    # first line has headers
    return count - 1

def add_json_combine_job(dax, prefix, chunk, job_number, start_num):
    """
    adds a json combine job
     # application 1: readDLs startNum endNum Y.out
    """
    j = Job(name="JSON-COMBINE")
    out_file = File("%s-combined-%d.out" %(prefix, job_number))
    
    j.uses(out_file, link=Link.OUTPUT, transfer=False, register=False)
    end_num = start_num - 1

    for f in chunk:
        j.uses(f, link=Link.INPUT)
        end_num += 1

    # sanity check - make sure start and end bids match the chunk
    if chunk[0].name != "%d-DL.json"%(start_num):
        print("ERROR: Chunk start mismatch %s != %d-DL.json" %(chunk[0].name, start_num))
        print(chunk[0].name, chunk[-1].name)
        sys.exit(1)
    if chunk[-1].name != "%d-DL.json"%(end_num):
        print("ERROR: Chunk end mismatch %s != %d-DL.json" %(chunk[-1].name, end_num))
        print(chunk[0].name, chunk[-1].name)
        sys.exit(1)
    
    args = "%d %d" %(start_num, end_num)
    j.addArguments(args)
    j.addArguments(out_file)
    
    dax.addJob(j)
    return out_file


def combine_json_outputs(dax, prefix, first_bid, outputs):
    """
    creates a set of jobs that combine json outputs and generate
    csv files
    # application 1: readDLs startNum endNum Y.out
    """
    max_files = 25
    new_outputs = []
    output_chunks = [outputs[i:i + max_files] for i in xrange(0, len(outputs), max_files)]
    job_count = 0
    start_num = first_bid
    
    exe = Executable(name="JSON-COMBINE", arch="x86_64", installed=False)
    exe.addPFN(local_pfn(top_dir + "/finalProcessing/readDLs"))
    dax.addExecutable(exe)
    
    for chunk in output_chunks:
        job_count = job_count + 1
        f = add_json_combine_job(dax, prefix, chunk, job_count, start_num)
        start_num += max_files
        new_outputs.append(f)
    return new_outputs


def add_combine_job(dax, prefix, chunk, level, job_number, final):
    """
    adds a combine job
    """
    j = Job(name="COMBINE")
    out_file = File("%s-combined-%d-%d" %(prefix, level, job_number))
    if final:
        out_file = File(prefix + "-combined")
    j.uses(out_file, link=Link.OUTPUT, transfer=final, register=False)
    j.addArguments(out_file)
    for f in chunk:
        j.uses(f, link=Link.INPUT)
        j.addArguments(f)
    dax.addJob(j)
    return out_file


def combine_outputs(dax, prefix, outputs, level):
    """
    creates a set of small jobs to combine the results
    """
    max_files = 25
    new_outputs = []

    output_chunks = [outputs[i:i + max_files] for i in xrange(0, len(outputs), max_files)]

    job_count = 0
    for chunk in output_chunks:
        job_count = job_count + 1
        f = add_combine_job(dax, prefix, chunk, level, job_count, False)
        new_outputs.append(f)

    # end condition - only one chunk
    if len(new_outputs) <= max_files:
        return add_combine_job(dax, prefix, new_outputs, level + 1, 1, True)

    return combine_outputs(dax, prefix, new_outputs, level + 1)


def generate_subwf(wfid, first_bid, last_bid):

    global run_dir

    task_files = {}

    # Create a abstract dag
    dax = ADAG("%06d" %(wfid))

    add_executables(dax)
    
    # how many buildings to group together
    buildings_group_size = 500
        
    count = 0
    count_label_event = 0
    count_label = 0

    dls_to_combine = []
    
    for building_base in range(first_bid, last_bid + 1, buildings_group_size):
   
        inner_last_bid = min(last_bid, building_base + buildings_group_size - 1)
    
        ###################################################################
        # createBIM
    
        bim = Job(name="BIM")
        add_task_files(task_files, dax, bim, "createBIM", top_dir + "/createBIM")
        add_task_files(task_files, dax, bim, "parcels.csv", top_dir + "/workflow/inputs")
        add_task_files(task_files, dax, bim, "buildings.csv", top_dir + "/workflow/inputs")
        bim.addArguments(str(building_base), str(inner_last_bid))
    
        # inputs
        for f in task_files["BIM"]:
            bim.uses(f, link=Link.INPUT)
    
        # outputs
        bim_jsons = {}
        for i in range(building_base, inner_last_bid + 1):
            bim_json = File("%d-BIM.json" %(i))
            bim.uses(bim_json, link=Link.OUTPUT, transfer=False, register=False)
            bim_jsons[i] = bim_json
    
        dax.addJob(bim)

        for building_id in range(building_base, inner_last_bid + 1):

            # label based clustering - these tasks are so short we want to
            # bundle a bunch of buildings together, but only 50 buildings
            # at the time
            if count % 200 == 0:
                count_label_event += 1
            if count % 100 == 0:
                count_label += 1
            label_event = "event%05d" %(count_label_event)
            label = "sim%05d" %(count_label)
   
            count += 1
    
            ###################################################################
            # createEvent
        
            event = Job(name="EVENT")
            event.addProfile(Profile("dagman", "CATEGORY", "event"))
            event.profile(namespace="pegasus", key="label", value=label_event)
            add_task_files(task_files, dax, event, "createEVENT", top_dir + "/createEVENT")
            add_task_files(task_files, dax, event, "HFmeta", top_dir + "/createEVENT")
            add_task_files(task_files, dax, event, "motions.tar.gz", top_dir + "/workflow/inputs")
            event.addArguments(str(building_id))
        
            # inputs
            for f in task_files["EVENT"]:
                event.uses(f, link=Link.INPUT)
            event.uses(bim_jsons[building_id], link=Link.INPUT)
        
            # outputs
            event_json = File("%d-EVENT.json" %(building_id))
            event.uses(event_json, link=Link.OUTPUT, transfer=False, register=False)
        
            dax.addJob(event)
            dax.depends(parent=bim, child=event)
        
            ###################################################################
            # createSAM
        
            sam = Job(name="SAM")
            sam.profile(namespace="pegasus", key="label", value=label)
            add_task_files(task_files, dax, sam, "createSAM", top_dir + "/createSAM")
            add_task_files(task_files, dax, sam, "HazusData.txt", top_dir + "/createSAM/data")
            sam.addArguments(str(building_id))
            
            # inputs
            for f in task_files["SAM"]:
                sam.uses(f, link=Link.INPUT)
            sam.uses(bim_jsons[building_id], link=Link.INPUT)
            sam.uses(event_json, link=Link.INPUT)
        
            # outputs
            sam_json = File("%d-SAM.json" %(building_id))
            sam.uses(sam_json, link=Link.OUTPUT, transfer=False, register=False)
        
            dax.addJob(sam)
            dax.depends(parent=bim, child=sam)
            dax.depends(parent=event, child=sam)
        
            ###################################################################
            # createEDP
        
            edp = Job(name="EDP")
            edp.profile(namespace="pegasus", key="label", value=label)
            add_task_files(task_files, dax, edp, "createEDP", top_dir + "/createEDP")
            edp.addArguments(str(building_id))
            
            # inputs
            for f in task_files["EDP"]:
                edp.uses(f, link=Link.INPUT)
            edp.uses(bim_jsons[building_id], link=Link.INPUT)
            edp.uses(event_json, link=Link.INPUT)
            edp.uses(sam_json, link=Link.INPUT)
        
            # outputs
            edp_json = File("%d-EDP.json" %(building_id))
            edp.uses(edp_json, link=Link.OUTPUT, transfer=False, register=False)
        
            dax.addJob(edp)
            dax.depends(parent=bim, child=edp)
            dax.depends(parent=event, child=edp)
            dax.depends(parent=sam, child=edp)
        
            ###################################################################
            # performSIMULATION
        
            simulation = Job(name="SIMULATION")
            simulation.profile(namespace="pegasus", key="label", value=label)
            add_task_files(task_files, dax, simulation, "mainPreprocessor", top_dir + "/performSIMULATION")
            add_task_files(task_files, dax, simulation, "mainPostprocessor", top_dir + "/performSIMULATION")
            add_task_files(task_files, dax, simulation, "OpenSees", top_dir + "/workflow/task-files/SIMULATION")
            simulation.addArguments(str(building_id))
            
            # inputs
            for f in task_files["SIMULATION"]:
                simulation.uses(f, link=Link.INPUT)
            simulation.uses(bim_jsons[building_id], link=Link.INPUT)
            simulation.uses(event_json, link=Link.INPUT)
            simulation.uses(sam_json, link=Link.INPUT)
            simulation.uses(edp_json, link=Link.INPUT)
        
            # outputs
            simulation_json = File("%d-SIMULATION.json" %(building_id))
            simulation.uses(simulation_json, link=Link.OUTPUT, transfer=False, register=False)
        
            dax.addJob(simulation)
            dax.depends(parent=bim, child=simulation)
            dax.depends(parent=event, child=simulation)
            dax.depends(parent=sam, child=simulation)
            dax.depends(parent=edp, child=simulation)

            ###################################################################
            # createDL
        
            dl = Job(name="DL")
            dl.profile(namespace="pegasus", key="label", value=label)
            add_task_files(task_files, dax, dl, "createLOSS", top_dir + "/createDL")
            add_task_files(task_files, dax, dl, "DL-data.tar.gz", top_dir + "/createDL")
            dl.addArguments(str(building_id))
            
            # inputs
            for f in task_files["DL"]:
                dl.uses(f, link=Link.INPUT)
            dl.uses(bim_jsons[building_id], link=Link.INPUT)
            dl.uses(simulation_json, link=Link.INPUT)
        
            # outputs
            dl_json = File("%d-DL.json" %(building_id))
            dl.uses(dl_json, link=Link.OUTPUT, transfer=False, register=False)
            
            dax.addJob(dl)
            dax.depends(parent=bim, child=dl)
            dax.depends(parent=simulation, child=dl)
            
            # keep track of outputs so we can combine
            dls_to_combine.append(dl_json)
            
   
    # first level of combine has to use json combine code to create csv files
    csv_to_combine = combine_json_outputs(dax, "%06d"%(wfid), first_bid, dls_to_combine) 

    # combine the outputs
    final_out_file = combine_outputs(dax, "%06d"%(wfid), csv_to_combine, 0) 
    
    # Write the DAX
    f = open("%s/%06d.xml" %(run_dir, wfid), "w")
    dax.writeXML(f)
    f.close()


def generate_combine_wf(num_wfs):

    global run_dir

    task_files = {}

    # Create a abstract dag
    dax = ADAG("combine")
    
    add_executables(dax)

    input_files = []
    for i in range(1, num_wfs + 1):
        f = File("%06d-subwf-result" %(i))
        f.addPFN(local_pfn("%s/outputs/%06d-combined" %(run_dir, i)))
        dax.addFile(f)
        input_files.append(f)

    combine_outputs(dax, "final", input_files, 0)

    # Write the DAX
    f = open("%s/combine.xml" %(run_dir), "w")
    dax.writeXML(f)
    f.close()


# condorpool is the default execenv
exec_site = "condorpool"
if len(sys.argv) == 2:
    exec_site = sys.argv[1]

# we need the top dir to the git checkout
top_dir = os.path.dirname(sys.argv[0])
top_dir = os.path.abspath(top_dir + "/..")

# check inputs
if not os.path.exists(top_dir + "/workflow/sites/" + exec_site + "/sites.xml"):
    print("Please specify a valid exec environment. Example: ./submit-workflow condorpool")
    sys.exit(1)
if not os.path.exists(top_dir + "/workflow/inputs/buildings.csv"):
    print("Please put a buildings.csv file in the inputs/ directory")
    sys.exit(1)
if not os.path.exists(top_dir + "/workflow/inputs/parcels.csv"):
    print("Please put a parcels.csv file in the inputs/ directory")
    sys.exit(1)

# set up run id and work dir
run_id = datetime.datetime.now().strftime("%Y%m%d%H%M%S")
run_dir = "/local-scratch/" + getpass.getuser() + "/workflows/" + run_id
os.makedirs(run_dir)

# prepare inputs
cmd = "cd " + top_dir + "/createDL && tar czf DL-data.tar.gz data"
call(cmd, shell=True)

# pegasus files
os.environ['RUN_DIR'] = run_dir
cmd = "envsubst <" + top_dir + "/workflow/sites/" + exec_site + "/sites.xml >" + run_dir + "/sites.xml"
call(cmd, shell=True)

cmd = "cp " + top_dir + "/workflow/sites/" + exec_site + "/pegasus.conf " + run_dir
call(cmd, shell=True)
#f = open(run_dir + "/pegasus.conf", "a")
#f.write("pegasus.catalog.replica.db.url=jdbc:sqlite:///" + run_dir + "/pegasus.sqlite")
#f.close()
#cmd = "pegasus-db-admin create sqlite:///" + run_dir + "/pegasus.sqlite"
#call(cmd, shell=True)

buildings_total = get_building_count()
buildings_per_subwf = 50000

print("Generating a hierarchical workflow for %d buildings..." %(buildings_total))
    
# Create a abstract dag
dax = ADAG("designsafe")

# email notificiations for when the state of the workflow changes
email_cmd = "/usr/share/pegasus/notification/email"
if getpass.getuser() == "rynge":
    email_cmd = email_cmd + " --to rynge@isi.edu"
dax.invoke('all',  email_cmd)

# Add executables to the DAX-level replica catalog
add_executables(dax)

task_files = {}

subwfs = []
subwf_id = 1

for subwf_first_id in range(1, buildings_total + 1, buildings_per_subwf):
    subwf_last_id = subwf_first_id + buildings_per_subwf - 1
    subwf_last_id = min(subwf_last_id, buildings_total)

    print("... adding sub workflow for %d .. %d" %(subwf_first_id, subwf_last_id))

    generate_subwf(subwf_id, subwf_first_id, subwf_last_id)

    subwf_dax = File("%06d.xml" %(subwf_id))
    subwf_dax.addPFN(PFN("%s/%06d.xml" %(run_dir, subwf_id), "local"))
    dax.addFile(subwf_dax)

    job = DAX("%06d.xml" %(subwf_id))
    job.addProfile(Profile("dagman", "CATEGORY", "subwf"))
    job.uses(subwf_dax)
    job.addArguments("-Dpegasus.catalog.site.file=%s/sites.xml" % (run_dir),
                     "--sites", "compute",
                     "--output-site", "local",
                     "--basename", "%06d" %(subwf_id),
                     "--cluster", "label",
                     "--cleanup", "inplace",
                     "--force")
    dax.addDAX(job)

    subwfs.append(job)

    subwf_id += 1

# we need one more sub workflow to do the final combining
generate_combine_wf(len(subwfs))
    
subwf_dax = File("combine.xml")
subwf_dax.addPFN(PFN("%s/combine.xml" %(run_dir), "local"))
dax.addFile(subwf_dax)

job = DAX("combine.xml")
job.uses(subwf_dax)
job.addArguments("-Dpegasus.catalog.site.file=%s/sites.xml" % (run_dir),
                 "--sites", "compute",
                 "--output-site", "local",
                 "--basename", "combine",
                 "--cleanup", "inplace",
                 "--force")
dax.addDAX(job)
# all other sub workflows are parents
for p in subwfs:
    dax.depends(parent=p, child=job)

# Write the DAX 
f = open("%s/dax.xml" %(run_dir), "w")
dax.writeXML(f)
f.close()

print("")
print("Outputs will show up in: " + run_dir + "/outputs/")
print("")

# environment needed for the site catalog
os.environ["RUN_DIR"] = run_dir

cmd = "pegasus-plan" + \
      " -Dpegasus.catalog.site.file=" + run_dir + "/sites.xml" + \
      " --conf " + run_dir + "/pegasus.conf" + \
      " --relative-dir " + run_id + \
      " --sites compute" + \
      " --output-site local" + \
      " --cleanup leaf" + \
      " --dir " + run_dir + \
      " --dax " + run_dir + "/dax.xml" + \
      " --submit"
call(cmd, shell=True)

sys.exit(0)

